\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}

\title{Performance Analysis -- Parallel Matrix Multiplication}
\author{Nirajan Khatri}
\date{\today}

\begin{document}
\maketitle

\section*{Analysis of row\_wise\_matrix\_mult.c}

The reference implementation uses a master-worker pattern where rank~0 acts as a dispatcher and performs no computation itself.
Several design decisions lead to poor scalability:

\begin{enumerate}
    \item \textbf{Row-by-row broadcast of B:}
        Matrix~$B$ is broadcast one row at a time ($n$ separate \texttt{MPI\_Bcast} calls), resulting in $O(n)$ latency-bound messages instead of a single bulk operation.
        As discussed in Lecture~11, collective operations should operate on large data chunks for efficiency.
    \item \textbf{Master bottleneck (Late Sender/Receiver):}
        All rows of $A$ are sent individually by rank~0 to workers via blocking \texttt{MPI\_Send}/\texttt{MPI\_Recv}.
        This creates a centralized communication pattern prone to Late Sender and Late Receiver problems (Lecture~12), where workers idle waiting for rows from the master.
    \item \textbf{Fine granularity:}
        Each message carries exactly one row ($n$ doubles), so message startup latency dominates for small to moderate $n$.
    \item \textbf{Idle master:}
        Rank~0 only dispatches work and collects results; it wastes one core that could contribute to computation.
    \item \textbf{No overlap of communication and computation:}
        All communication is blocking (Lecture~10), preventing any overlap of computation with data transfer.
    \item \textbf{Memory concentration:}
        Rank~0 allocates three full $n \times n$ matrices ($A$, $B$, and $C$), which limits scalability for large $n$.
\end{enumerate}

These issues would be clearly visible when profiling with tools such as Score-P, Scalasca, or EduMPI (Lecture~12, Exercise~12): the master rank would show high communication time, and worker ranks would show significant idle time in \texttt{MPI\_Recv}.

\section*{Analysis of Your Implementation(s)}

Our implementation (\texttt{matmul.c}) uses a row-block distribution strategy that addresses all identified weaknesses of the reference by applying MPI concepts from Lectures~9--12:

\begin{itemize}
    \item \textbf{Local initialization (no communication):}
        Every rank generates its own rows of $A$ and the full matrix $B$ independently using the deterministic RNG.
        This eliminates all initialization communication---no \texttt{MPI\_Bcast} calls needed.
    \item \textbf{All ranks compute (no idle master):}
        Work is divided evenly using the $\lfloor n/p \rfloor + 1$ pattern from Exercise~11 Task~4, with the first $n \bmod p$ ranks receiving one extra row.
    \item \textbf{Cache-efficient multiplication:}
        The \textit{i-k-j} loop order ensures sequential access to $B$ (row-major), providing excellent cache locality.
    \item \textbf{Collective gather via \texttt{MPI\_Gatherv} (Lecture 11):}
        The variable-count gather (\texttt{MPI\_Gatherv}) collects uneven row blocks at rank~0 in a single optimized collective call, replacing the $2n$ individual point-to-point messages used by the reference.
    \item \textbf{Deadlock-free (Lecture 10):}
        Only collective operations are used---no circular point-to-point dependencies that could cause deadlocks.
    \item \textbf{Early memory release:}
        Matrices $A$ and $B$ are freed immediately after multiplication completes, reducing peak memory usage.
\end{itemize}

\subsection*{Speedup Measurements}

All measurements use $n = 8000$, seed $= 42$, verbose $= 0$.
Each configuration was run 3 times, and the average execution time is reported.

\begin{table}[h]
\centering
\begin{tabular}{rrrrr}
\toprule
Nodes & MPI Processes & Avg.\ Time (s) & Speedup & Efficiency \\
\midrule
1 & 64  & 51.46 & 1.00 & 100.0\% \\
2 & 128 & 28.48 & 1.81 & 90.5\%  \\
4 & 256 & 16.97 & 3.03 & 75.8\%  \\
6 & 384 & 12.83 & 4.01 & 66.8\%  \\
8 & 512 & 10.97 & 4.69 & 58.6\%  \\
\bottomrule
\end{tabular}
\caption{Execution times and speedup for matmul (Row-Block), $n=8000$, seed$=42$.}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{speedup_plot.png}
    \caption{Speedup comparison for $n=8000$ (64 processes per node).}
\end{figure}

\subsection*{Discussion}

\textbf{Scalability:}
The row-block strategy achieves a speedup of $4.69\times$ on 8 nodes.
Efficiency decreases from 90.5\% (2 nodes) to 58.6\% (8 nodes), which is expected: with more nodes, inter-node communication in \texttt{MPI\_Gatherv} grows and each rank's workload shrinks, making the communication-to-computation ratio less favorable.

\textbf{Comparison with row\_wise\_matrix\_mult.c:}
The reference implementation funnels all communication through rank~0 in a centralized star topology, creating a sequential bottleneck that worsens with increasing process counts.
Our row-block strategy eliminates this bottleneck through local initialization and a single collective gather.

\textbf{Memory Efficiency:}
Both implementations require each rank to hold the full matrix~$B$ ($O(n^2)$ per rank).
However, the row-block strategy distributes $A$ and $C$ across ranks ($O(n^2/p)$ each), whereas the reference concentrates all three full matrices on rank~0.

\textbf{MPI Communication:}
The row-block strategy uses a single \texttt{MPI\_Gatherv} collective ($O(n^2)$ data moved via optimized tree-based algorithm).
The reference uses $n$ individual \texttt{MPI\_Bcast} calls for $B$ plus $2n$ point-to-point messages for $A$ rows and results, leading to much higher message overhead and latency.

\end{document}
